{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7451fd50-aa46-4264-8ddd-bbffeb6708b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import optuna\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten,  Dropout, SpatialDropout2D\n",
    "from tensorflow.keras.optimizers import SGD, Adam, RMSprop, Adagrad, Adadelta, Nadam\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a2a8e5-1000-4b09-9233-391e71fadfba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.20.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8132cb1-b462-4d71-a7ac-a74cc3051270",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'C:\\Users\\admin\\Downloads\\application.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e9757c53-01d1-4e95-b71f-df65a0108dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[['SK_ID_CURR', 'ORGANIZATION_TYPE']], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d121bd06-c514-47f2-9d9e-795b57ec74ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['AGE'] = (-data['DAYS_BIRTH'] / 365).astype(int)\n",
    "data = data.drop(['DAYS_BIRTH'], axis= 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d868981c-5bc2-4cc8-b1a3-dfd5ac77f0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_mapping = {\n",
    "    \"Lower secondary\": 1,\n",
    "    \"Secondary / secondary special\": 2,\n",
    "    \"Incomplete higher\": 3,\n",
    "    \"Higher education\": 4,\n",
    "    \"Academic degree\": 5,\n",
    "}\n",
    "data[\"NAME_EDUCATION_TYPE\"] = data[\"NAME_EDUCATION_TYPE\"].map(segment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d88e476-f3a5-4d83-b89a-ad04458d382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['CODE_GENDER'] = data['CODE_GENDER'].replace('XNA', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7ba9b2-04e7-4c75-8b79-74735a6135b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "cols_to_drop = data.columns[data.isnull().sum() / len(data) > threshold].tolist()\n",
    "\n",
    "cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165e1495-e646-4677-bb7e-538a3fe05066",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c83fbfa-332e-4792-ad12-8488f06e495e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_12352\\1157204993.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  data[i].fillna(data[i].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "for i in data.columns:\n",
    "    if data[i].dtype == 'object':\n",
    "        data[i].fillna(data[i].mode()[0], inplace=True)\n",
    "    else:\n",
    "        data[i].fillna(data[i].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2824d29a-6447-415c-9270-0361e7b5e6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoders = {}\n",
    "for i in data.select_dtypes(include=['object']).columns:\n",
    "    le = LabelEncoder()\n",
    "    data[i] = le.fit_transform(data[i])\n",
    "    encoders[i] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e3b01b37-2377-41b6-8e84-d6028c7e12d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in data:\n",
    "    print(data[i].isnull().sum())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "908f052a-2cc2-4138-98a6-3a1e8a7c1a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>...</th>\n",
       "      <th>FLAG_DOCUMENT_19</th>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "      <th>AGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006402</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.034362</td>\n",
       "      <td>0.267395</td>\n",
       "      <td>0.265474</td>\n",
       "      <td>1.899974</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307506</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>254700.0</td>\n",
       "      <td>27558.0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006402</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.034362</td>\n",
       "      <td>0.267395</td>\n",
       "      <td>0.265474</td>\n",
       "      <td>1.899974</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307507</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>72000.0</td>\n",
       "      <td>269550.0</td>\n",
       "      <td>12001.5</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.006402</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.034362</td>\n",
       "      <td>0.267395</td>\n",
       "      <td>0.265474</td>\n",
       "      <td>1.899974</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307508</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>153000.0</td>\n",
       "      <td>677664.0</td>\n",
       "      <td>29979.0</td>\n",
       "      <td>585000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307509</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>171000.0</td>\n",
       "      <td>370107.0</td>\n",
       "      <td>20205.0</td>\n",
       "      <td>319500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307510</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>675000.0</td>\n",
       "      <td>49117.5</td>\n",
       "      <td>675000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307511 rows × 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TARGET  NAME_CONTRACT_TYPE  CODE_GENDER  FLAG_OWN_CAR  \\\n",
       "0            1                   0            1             0   \n",
       "1            0                   0            0             0   \n",
       "2            0                   1            1             1   \n",
       "3            0                   0            0             0   \n",
       "4            0                   0            1             0   \n",
       "...        ...                 ...          ...           ...   \n",
       "307506       0                   0            1             0   \n",
       "307507       0                   0            0             0   \n",
       "307508       0                   0            0             0   \n",
       "307509       1                   0            0             0   \n",
       "307510       0                   0            0             0   \n",
       "\n",
       "        FLAG_OWN_REALTY  CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  \\\n",
       "0                     1             0          202500.0    406597.5   \n",
       "1                     0             0          270000.0   1293502.5   \n",
       "2                     1             0           67500.0    135000.0   \n",
       "3                     1             0          135000.0    312682.5   \n",
       "4                     1             0          121500.0    513000.0   \n",
       "...                 ...           ...               ...         ...   \n",
       "307506                0             0          157500.0    254700.0   \n",
       "307507                1             0           72000.0    269550.0   \n",
       "307508                1             0          153000.0    677664.0   \n",
       "307509                1             0          171000.0    370107.0   \n",
       "307510                0             0          157500.0    675000.0   \n",
       "\n",
       "        AMT_ANNUITY  AMT_GOODS_PRICE  ...  FLAG_DOCUMENT_19  FLAG_DOCUMENT_20  \\\n",
       "0           24700.5         351000.0  ...                 0                 0   \n",
       "1           35698.5        1129500.0  ...                 0                 0   \n",
       "2            6750.0         135000.0  ...                 0                 0   \n",
       "3           29686.5         297000.0  ...                 0                 0   \n",
       "4           21865.5         513000.0  ...                 0                 0   \n",
       "...             ...              ...  ...               ...               ...   \n",
       "307506      27558.0         225000.0  ...                 0                 0   \n",
       "307507      12001.5         225000.0  ...                 0                 0   \n",
       "307508      29979.0         585000.0  ...                 0                 0   \n",
       "307509      20205.0         319500.0  ...                 0                 0   \n",
       "307510      49117.5         675000.0  ...                 0                 0   \n",
       "\n",
       "        FLAG_DOCUMENT_21  AMT_REQ_CREDIT_BUREAU_HOUR  \\\n",
       "0                      0                    0.000000   \n",
       "1                      0                    0.000000   \n",
       "2                      0                    0.000000   \n",
       "3                      0                    0.006402   \n",
       "4                      0                    0.000000   \n",
       "...                  ...                         ...   \n",
       "307506                 0                    0.006402   \n",
       "307507                 0                    0.006402   \n",
       "307508                 0                    1.000000   \n",
       "307509                 0                    0.000000   \n",
       "307510                 0                    0.000000   \n",
       "\n",
       "        AMT_REQ_CREDIT_BUREAU_DAY  AMT_REQ_CREDIT_BUREAU_WEEK  \\\n",
       "0                           0.000                    0.000000   \n",
       "1                           0.000                    0.000000   \n",
       "2                           0.000                    0.000000   \n",
       "3                           0.007                    0.034362   \n",
       "4                           0.000                    0.000000   \n",
       "...                           ...                         ...   \n",
       "307506                      0.007                    0.034362   \n",
       "307507                      0.007                    0.034362   \n",
       "307508                      0.000                    0.000000   \n",
       "307509                      0.000                    0.000000   \n",
       "307510                      0.000                    0.000000   \n",
       "\n",
       "        AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  \\\n",
       "0                        0.000000                   0.000000   \n",
       "1                        0.000000                   0.000000   \n",
       "2                        0.000000                   0.000000   \n",
       "3                        0.267395                   0.265474   \n",
       "4                        0.000000                   0.000000   \n",
       "...                           ...                        ...   \n",
       "307506                   0.267395                   0.265474   \n",
       "307507                   0.267395                   0.265474   \n",
       "307508                   1.000000                   0.000000   \n",
       "307509                   0.000000                   0.000000   \n",
       "307510                   2.000000                   0.000000   \n",
       "\n",
       "        AMT_REQ_CREDIT_BUREAU_YEAR  AGE  \n",
       "0                         1.000000   25  \n",
       "1                         0.000000   45  \n",
       "2                         0.000000   52  \n",
       "3                         1.899974   52  \n",
       "4                         0.000000   54  \n",
       "...                            ...  ...  \n",
       "307506                    1.899974   25  \n",
       "307507                    1.899974   56  \n",
       "307508                    1.000000   41  \n",
       "307509                    0.000000   32  \n",
       "307510                    1.000000   46  \n",
       "\n",
       "[307511 rows x 120 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.get_dummies(data, drop_first=True)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "65e5eae5-544a-4cbc-a9b7-0132e62b3597",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = data['TARGET']\n",
    "\n",
    "inputs = data.drop(['TARGET'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a352b4b0-1f27-47f8-930e-013058902f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>NAME_TYPE_SUITE</th>\n",
       "      <th>...</th>\n",
       "      <th>FLAG_DOCUMENT_19</th>\n",
       "      <th>FLAG_DOCUMENT_20</th>\n",
       "      <th>FLAG_DOCUMENT_21</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_HOUR</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_DAY</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_WEEK</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_MON</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_QRT</th>\n",
       "      <th>AMT_REQ_CREDIT_BUREAU_YEAR</th>\n",
       "      <th>AGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.324395</td>\n",
       "      <td>1.388015</td>\n",
       "      <td>-0.717914</td>\n",
       "      <td>0.664531</td>\n",
       "      <td>-0.577538</td>\n",
       "      <td>0.142129</td>\n",
       "      <td>-0.478095</td>\n",
       "      <td>-0.166149</td>\n",
       "      <td>-0.507465</td>\n",
       "      <td>0.435227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024402</td>\n",
       "      <td>-0.022529</td>\n",
       "      <td>-0.018305</td>\n",
       "      <td>-8.210023e-02</td>\n",
       "      <td>-0.067957</td>\n",
       "      <td>-1.805048e-01</td>\n",
       "      <td>-0.313873</td>\n",
       "      <td>-3.594746e-01</td>\n",
       "      <td>-5.176655e-01</td>\n",
       "      <td>-1.542169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.324395</td>\n",
       "      <td>-0.720370</td>\n",
       "      <td>-0.717914</td>\n",
       "      <td>-1.504820</td>\n",
       "      <td>-0.577538</td>\n",
       "      <td>0.426792</td>\n",
       "      <td>1.725450</td>\n",
       "      <td>0.592677</td>\n",
       "      <td>1.600698</td>\n",
       "      <td>-2.380655</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024402</td>\n",
       "      <td>-0.022529</td>\n",
       "      <td>-0.018305</td>\n",
       "      <td>-8.210023e-02</td>\n",
       "      <td>-0.067957</td>\n",
       "      <td>-1.805048e-01</td>\n",
       "      <td>-0.313873</td>\n",
       "      <td>-3.594746e-01</td>\n",
       "      <td>-1.092866e+00</td>\n",
       "      <td>0.130831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.082659</td>\n",
       "      <td>1.388015</td>\n",
       "      <td>1.392925</td>\n",
       "      <td>0.664531</td>\n",
       "      <td>-0.577538</td>\n",
       "      <td>-0.427196</td>\n",
       "      <td>-1.152888</td>\n",
       "      <td>-1.404676</td>\n",
       "      <td>-1.092389</td>\n",
       "      <td>0.435227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024402</td>\n",
       "      <td>-0.022529</td>\n",
       "      <td>-0.018305</td>\n",
       "      <td>-8.210023e-02</td>\n",
       "      <td>-0.067957</td>\n",
       "      <td>-1.805048e-01</td>\n",
       "      <td>-0.313873</td>\n",
       "      <td>-3.594746e-01</td>\n",
       "      <td>-1.092866e+00</td>\n",
       "      <td>0.716381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.324395</td>\n",
       "      <td>-0.720370</td>\n",
       "      <td>-0.717914</td>\n",
       "      <td>0.664531</td>\n",
       "      <td>-0.577538</td>\n",
       "      <td>-0.142533</td>\n",
       "      <td>-0.711430</td>\n",
       "      <td>0.177869</td>\n",
       "      <td>-0.653696</td>\n",
       "      <td>0.435227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024402</td>\n",
       "      <td>-0.022529</td>\n",
       "      <td>-0.018305</td>\n",
       "      <td>3.336720e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.645032e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.516682e-17</td>\n",
       "      <td>-3.831603e-16</td>\n",
       "      <td>0.716381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.324395</td>\n",
       "      <td>1.388015</td>\n",
       "      <td>-0.717914</td>\n",
       "      <td>0.664531</td>\n",
       "      <td>-0.577538</td>\n",
       "      <td>-0.199466</td>\n",
       "      <td>-0.213734</td>\n",
       "      <td>-0.361755</td>\n",
       "      <td>-0.068772</td>\n",
       "      <td>0.435227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024402</td>\n",
       "      <td>-0.022529</td>\n",
       "      <td>-0.018305</td>\n",
       "      <td>-8.210023e-02</td>\n",
       "      <td>-0.067957</td>\n",
       "      <td>-1.805048e-01</td>\n",
       "      <td>-0.313873</td>\n",
       "      <td>-3.594746e-01</td>\n",
       "      <td>-1.092866e+00</td>\n",
       "      <td>0.883681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307506</th>\n",
       "      <td>-0.324395</td>\n",
       "      <td>1.388015</td>\n",
       "      <td>-0.717914</td>\n",
       "      <td>-1.504820</td>\n",
       "      <td>-0.577538</td>\n",
       "      <td>-0.047646</td>\n",
       "      <td>-0.855489</td>\n",
       "      <td>0.031009</td>\n",
       "      <td>-0.848671</td>\n",
       "      <td>0.435227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024402</td>\n",
       "      <td>-0.022529</td>\n",
       "      <td>-0.018305</td>\n",
       "      <td>3.336720e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.645032e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.516682e-17</td>\n",
       "      <td>-3.831603e-16</td>\n",
       "      <td>-1.542169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307507</th>\n",
       "      <td>-0.324395</td>\n",
       "      <td>-0.720370</td>\n",
       "      <td>-0.717914</td>\n",
       "      <td>0.664531</td>\n",
       "      <td>-0.577538</td>\n",
       "      <td>-0.408219</td>\n",
       "      <td>-0.818594</td>\n",
       "      <td>-1.042339</td>\n",
       "      <td>-0.848671</td>\n",
       "      <td>0.435227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024402</td>\n",
       "      <td>-0.022529</td>\n",
       "      <td>-0.018305</td>\n",
       "      <td>3.336720e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3.645032e-17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-7.516682e-17</td>\n",
       "      <td>-3.831603e-16</td>\n",
       "      <td>1.050981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307508</th>\n",
       "      <td>-0.324395</td>\n",
       "      <td>-0.720370</td>\n",
       "      <td>-0.717914</td>\n",
       "      <td>0.664531</td>\n",
       "      <td>-0.577538</td>\n",
       "      <td>-0.066623</td>\n",
       "      <td>0.195379</td>\n",
       "      <td>0.198050</td>\n",
       "      <td>0.126202</td>\n",
       "      <td>0.435227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024402</td>\n",
       "      <td>-0.022529</td>\n",
       "      <td>-0.018305</td>\n",
       "      <td>1.274116e+01</td>\n",
       "      <td>-0.067957</td>\n",
       "      <td>-1.805048e-01</td>\n",
       "      <td>0.859944</td>\n",
       "      <td>-3.594746e-01</td>\n",
       "      <td>-5.176655e-01</td>\n",
       "      <td>-0.203769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307509</th>\n",
       "      <td>-0.324395</td>\n",
       "      <td>-0.720370</td>\n",
       "      <td>-0.717914</td>\n",
       "      <td>0.664531</td>\n",
       "      <td>-0.577538</td>\n",
       "      <td>0.009287</td>\n",
       "      <td>-0.568757</td>\n",
       "      <td>-0.476324</td>\n",
       "      <td>-0.592767</td>\n",
       "      <td>0.435227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024402</td>\n",
       "      <td>-0.022529</td>\n",
       "      <td>-0.018305</td>\n",
       "      <td>-8.210023e-02</td>\n",
       "      <td>-0.067957</td>\n",
       "      <td>-1.805048e-01</td>\n",
       "      <td>-0.313873</td>\n",
       "      <td>-3.594746e-01</td>\n",
       "      <td>-1.092866e+00</td>\n",
       "      <td>-0.956619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307510</th>\n",
       "      <td>-0.324395</td>\n",
       "      <td>-0.720370</td>\n",
       "      <td>-0.717914</td>\n",
       "      <td>-1.504820</td>\n",
       "      <td>-0.577538</td>\n",
       "      <td>-0.047646</td>\n",
       "      <td>0.188760</td>\n",
       "      <td>1.518545</td>\n",
       "      <td>0.369920</td>\n",
       "      <td>0.435227</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024402</td>\n",
       "      <td>-0.022529</td>\n",
       "      <td>-0.018305</td>\n",
       "      <td>-8.210023e-02</td>\n",
       "      <td>-0.067957</td>\n",
       "      <td>-1.805048e-01</td>\n",
       "      <td>2.033760</td>\n",
       "      <td>-3.594746e-01</td>\n",
       "      <td>-5.176655e-01</td>\n",
       "      <td>0.214481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>307511 rows × 119 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        NAME_CONTRACT_TYPE  CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  \\\n",
       "0                -0.324395     1.388015     -0.717914         0.664531   \n",
       "1                -0.324395    -0.720370     -0.717914        -1.504820   \n",
       "2                 3.082659     1.388015      1.392925         0.664531   \n",
       "3                -0.324395    -0.720370     -0.717914         0.664531   \n",
       "4                -0.324395     1.388015     -0.717914         0.664531   \n",
       "...                    ...          ...           ...              ...   \n",
       "307506           -0.324395     1.388015     -0.717914        -1.504820   \n",
       "307507           -0.324395    -0.720370     -0.717914         0.664531   \n",
       "307508           -0.324395    -0.720370     -0.717914         0.664531   \n",
       "307509           -0.324395    -0.720370     -0.717914         0.664531   \n",
       "307510           -0.324395    -0.720370     -0.717914        -1.504820   \n",
       "\n",
       "        CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "0          -0.577538          0.142129   -0.478095    -0.166149   \n",
       "1          -0.577538          0.426792    1.725450     0.592677   \n",
       "2          -0.577538         -0.427196   -1.152888    -1.404676   \n",
       "3          -0.577538         -0.142533   -0.711430     0.177869   \n",
       "4          -0.577538         -0.199466   -0.213734    -0.361755   \n",
       "...              ...               ...         ...          ...   \n",
       "307506     -0.577538         -0.047646   -0.855489     0.031009   \n",
       "307507     -0.577538         -0.408219   -0.818594    -1.042339   \n",
       "307508     -0.577538         -0.066623    0.195379     0.198050   \n",
       "307509     -0.577538          0.009287   -0.568757    -0.476324   \n",
       "307510     -0.577538         -0.047646    0.188760     1.518545   \n",
       "\n",
       "        AMT_GOODS_PRICE  NAME_TYPE_SUITE  ...  FLAG_DOCUMENT_19  \\\n",
       "0             -0.507465         0.435227  ...         -0.024402   \n",
       "1              1.600698        -2.380655  ...         -0.024402   \n",
       "2             -1.092389         0.435227  ...         -0.024402   \n",
       "3             -0.653696         0.435227  ...         -0.024402   \n",
       "4             -0.068772         0.435227  ...         -0.024402   \n",
       "...                 ...              ...  ...               ...   \n",
       "307506        -0.848671         0.435227  ...         -0.024402   \n",
       "307507        -0.848671         0.435227  ...         -0.024402   \n",
       "307508         0.126202         0.435227  ...         -0.024402   \n",
       "307509        -0.592767         0.435227  ...         -0.024402   \n",
       "307510         0.369920         0.435227  ...         -0.024402   \n",
       "\n",
       "        FLAG_DOCUMENT_20  FLAG_DOCUMENT_21  AMT_REQ_CREDIT_BUREAU_HOUR  \\\n",
       "0              -0.022529         -0.018305               -8.210023e-02   \n",
       "1              -0.022529         -0.018305               -8.210023e-02   \n",
       "2              -0.022529         -0.018305               -8.210023e-02   \n",
       "3              -0.022529         -0.018305                3.336720e-17   \n",
       "4              -0.022529         -0.018305               -8.210023e-02   \n",
       "...                  ...               ...                         ...   \n",
       "307506         -0.022529         -0.018305                3.336720e-17   \n",
       "307507         -0.022529         -0.018305                3.336720e-17   \n",
       "307508         -0.022529         -0.018305                1.274116e+01   \n",
       "307509         -0.022529         -0.018305               -8.210023e-02   \n",
       "307510         -0.022529         -0.018305               -8.210023e-02   \n",
       "\n",
       "        AMT_REQ_CREDIT_BUREAU_DAY  AMT_REQ_CREDIT_BUREAU_WEEK  \\\n",
       "0                       -0.067957               -1.805048e-01   \n",
       "1                       -0.067957               -1.805048e-01   \n",
       "2                       -0.067957               -1.805048e-01   \n",
       "3                        0.000000               -3.645032e-17   \n",
       "4                       -0.067957               -1.805048e-01   \n",
       "...                           ...                         ...   \n",
       "307506                   0.000000               -3.645032e-17   \n",
       "307507                   0.000000               -3.645032e-17   \n",
       "307508                  -0.067957               -1.805048e-01   \n",
       "307509                  -0.067957               -1.805048e-01   \n",
       "307510                  -0.067957               -1.805048e-01   \n",
       "\n",
       "        AMT_REQ_CREDIT_BUREAU_MON  AMT_REQ_CREDIT_BUREAU_QRT  \\\n",
       "0                       -0.313873              -3.594746e-01   \n",
       "1                       -0.313873              -3.594746e-01   \n",
       "2                       -0.313873              -3.594746e-01   \n",
       "3                        0.000000              -7.516682e-17   \n",
       "4                       -0.313873              -3.594746e-01   \n",
       "...                           ...                        ...   \n",
       "307506                   0.000000              -7.516682e-17   \n",
       "307507                   0.000000              -7.516682e-17   \n",
       "307508                   0.859944              -3.594746e-01   \n",
       "307509                  -0.313873              -3.594746e-01   \n",
       "307510                   2.033760              -3.594746e-01   \n",
       "\n",
       "        AMT_REQ_CREDIT_BUREAU_YEAR       AGE  \n",
       "0                    -5.176655e-01 -1.542169  \n",
       "1                    -1.092866e+00  0.130831  \n",
       "2                    -1.092866e+00  0.716381  \n",
       "3                    -3.831603e-16  0.716381  \n",
       "4                    -1.092866e+00  0.883681  \n",
       "...                            ...       ...  \n",
       "307506               -3.831603e-16 -1.542169  \n",
       "307507               -3.831603e-16  1.050981  \n",
       "307508               -5.176655e-01 -0.203769  \n",
       "307509               -1.092866e+00 -0.956619  \n",
       "307510               -5.176655e-01  0.214481  \n",
       "\n",
       "[307511 rows x 119 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(inputs)\n",
    "\n",
    "scaled = scaler.transform(inputs)\n",
    "\n",
    "inputs_scaled = pd.DataFrame(scaled, columns=inputs.columns)\n",
    "\n",
    "inputs_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c2600cf7-5ce1-485d-ab04-255d258aa9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs_scaled, targets, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c6da51e1-f794-48d0-b8c3-840d9994463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(trial):\n",
    "    # Building artificial neural network\n",
    "    model = Sequential()\n",
    "\n",
    "     # we add 2 hidden layers and 1 output layer\n",
    "    model.add(Dense(units=trial.suggest_int('units_layer1', 6, 32), activation='relu'))\n",
    "    model.add(Dense(units=trial.suggest_int('units_layer2', 6, 32), activation='relu'))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "    # Suggest hyperparameters for the optimizer\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'sgd', 'rmsprop', 'adagrad'])\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
    "    \n",
    "    if optimizer_name == 'adam':\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'sgd':\n",
    "        optimizer = SGD(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'rmsprop':\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    elif optimizer_name == 'adagrad':\n",
    "        optimizer = Adagrad(learning_rate=learning_rate)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['AUC'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a509e90e-4227-407a-a159-a920a4570a3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 10:11:10,441] A new study created in memory with name: no-name-6b9e95c3-790f-48bc-97a2-503ff52d0a83\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_12352\\3644950783.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.6548 - loss: 0.2891\n",
      "Epoch 2/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7273 - loss: 0.2559\n",
      "Epoch 3/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7349 - loss: 0.2539\n",
      "Epoch 4/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7387 - loss: 0.2527\n",
      "Epoch 5/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7405 - loss: 0.2520\n",
      "Epoch 6/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7422 - loss: 0.2514\n",
      "Epoch 7/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7435 - loss: 0.2510\n",
      "Epoch 8/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.7443 - loss: 0.2506\n",
      "Epoch 9/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7453 - loss: 0.2504\n",
      "Epoch 10/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7457 - loss: 0.2501\n",
      "Epoch 11/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.7467 - loss: 0.2499\n",
      "Epoch 12/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7472 - loss: 0.2498\n",
      "Epoch 13/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7474 - loss: 0.2496\n",
      "Epoch 14/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7479 - loss: 0.2495\n",
      "Epoch 15/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7480 - loss: 0.2494\n",
      "Epoch 16/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7488 - loss: 0.2492\n",
      "Epoch 17/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.7489 - loss: 0.2491\n",
      "Epoch 18/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7495 - loss: 0.2490\n",
      "Epoch 19/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7492 - loss: 0.2490\n",
      "Epoch 20/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7496 - loss: 0.2489\n",
      "Epoch 21/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7500 - loss: 0.2487\n",
      "Epoch 22/22\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7500 - loss: 0.2487\n",
      "\u001b[1m1922/1922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 10:14:07,626] Trial 0 finished with value: 0.7444090078813803 and parameters: {'epochs': 22, 'batch_size': 56, 'units_layer1': 7, 'units_layer2': 32, 'optimizer': 'adam', 'learning_rate': 0.0002454790046811478}. Best is trial 0 with value: 0.7444090078813803.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_12352\\3644950783.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7065 - loss: 0.2657\n",
      "Epoch 2/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7385 - loss: 0.2523\n",
      "Epoch 3/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7441 - loss: 0.2506\n",
      "Epoch 4/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.7475 - loss: 0.2496\n",
      "Epoch 5/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7492 - loss: 0.2490\n",
      "Epoch 6/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7519 - loss: 0.2482\n",
      "Epoch 7/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7532 - loss: 0.2478\n",
      "Epoch 8/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7549 - loss: 0.2473\n",
      "Epoch 9/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7558 - loss: 0.2470\n",
      "Epoch 10/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - AUC: 0.7574 - loss: 0.2465\n",
      "Epoch 11/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7585 - loss: 0.2462\n",
      "Epoch 12/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7594 - loss: 0.2459\n",
      "Epoch 13/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7603 - loss: 0.2456\n",
      "Epoch 14/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7615 - loss: 0.2452\n",
      "Epoch 15/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7624 - loss: 0.2449\n",
      "Epoch 16/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7627 - loss: 0.2448\n",
      "Epoch 17/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7636 - loss: 0.2445\n",
      "Epoch 18/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7644 - loss: 0.2442\n",
      "Epoch 19/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.7640 - loss: 0.2442\n",
      "Epoch 20/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7649 - loss: 0.2439\n",
      "Epoch 21/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7657 - loss: 0.2437\n",
      "Epoch 22/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7656 - loss: 0.2436\n",
      "Epoch 23/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7660 - loss: 0.2434\n",
      "Epoch 24/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7666 - loss: 0.2434\n",
      "Epoch 25/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7674 - loss: 0.2431\n",
      "Epoch 26/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7679 - loss: 0.2430\n",
      "Epoch 27/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7678 - loss: 0.2429\n",
      "Epoch 28/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7687 - loss: 0.2426\n",
      "Epoch 29/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7685 - loss: 0.2426\n",
      "Epoch 30/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7689 - loss: 0.2425\n",
      "Epoch 31/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7691 - loss: 0.2424\n",
      "Epoch 32/32\n",
      "\u001b[1m4101/4101\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7700 - loss: 0.2422\n",
      "\u001b[1m1922/1922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 886us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 10:17:57,048] Trial 1 finished with value: 0.7367404179989931 and parameters: {'epochs': 32, 'batch_size': 60, 'units_layer1': 29, 'units_layer2': 11, 'optimizer': 'adam', 'learning_rate': 0.0006386456767190139}. Best is trial 0 with value: 0.7444090078813803.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_12352\\3644950783.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.5062 - loss: 0.4466\n",
      "Epoch 2/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.5398 - loss: 0.3093\n",
      "Epoch 3/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.5767 - loss: 0.2907\n",
      "Epoch 4/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.6070 - loss: 0.2829\n",
      "Epoch 5/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.6296 - loss: 0.2778\n",
      "Epoch 6/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.6469 - loss: 0.2742\n",
      "Epoch 7/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.6602 - loss: 0.2713\n",
      "Epoch 8/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.6708 - loss: 0.2691\n",
      "Epoch 9/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - AUC: 0.6793 - loss: 0.2673\n",
      "Epoch 10/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.6862 - loss: 0.2657\n",
      "Epoch 11/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.6919 - loss: 0.2645\n",
      "Epoch 12/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.6966 - loss: 0.2634\n",
      "Epoch 13/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.7006 - loss: 0.2625\n",
      "Epoch 14/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7039 - loss: 0.2617\n",
      "Epoch 15/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.7068 - loss: 0.2610\n",
      "Epoch 16/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7092 - loss: 0.2604\n",
      "Epoch 17/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7115 - loss: 0.2598\n",
      "Epoch 18/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7134 - loss: 0.2593\n",
      "Epoch 19/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7150 - loss: 0.2589\n",
      "Epoch 20/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7164 - loss: 0.2585\n",
      "Epoch 21/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7179 - loss: 0.2581\n",
      "Epoch 22/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7191 - loss: 0.2578\n",
      "Epoch 23/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7202 - loss: 0.2575\n",
      "Epoch 24/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7212 - loss: 0.2573\n",
      "Epoch 25/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7221 - loss: 0.2570\n",
      "Epoch 26/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7230 - loss: 0.2568\n",
      "Epoch 27/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7237 - loss: 0.2566\n",
      "Epoch 28/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7245 - loss: 0.2564\n",
      "Epoch 29/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7252 - loss: 0.2562\n",
      "Epoch 30/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7258 - loss: 0.2560\n",
      "Epoch 31/31\n",
      "\u001b[1m4473/4473\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7263 - loss: 0.2559\n",
      "\u001b[1m1922/1922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 10:21:53,063] Trial 2 finished with value: 0.7248279589057743 and parameters: {'epochs': 31, 'batch_size': 55, 'units_layer1': 29, 'units_layer2': 9, 'optimizer': 'sgd', 'learning_rate': 0.0005686526276834406}. Best is trial 0 with value: 0.7444090078813803.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_12352\\3644950783.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - AUC: 0.7179 - loss: 0.2614\n",
      "Epoch 2/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.7376 - loss: 0.2524\n",
      "Epoch 3/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - AUC: 0.7411 - loss: 0.2514\n",
      "Epoch 4/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - AUC: 0.7440 - loss: 0.2505\n",
      "Epoch 5/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.7456 - loss: 0.2500\n",
      "Epoch 6/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.7473 - loss: 0.2495\n",
      "Epoch 7/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.7487 - loss: 0.2491\n",
      "Epoch 8/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.7494 - loss: 0.2488\n",
      "Epoch 9/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - AUC: 0.7508 - loss: 0.2485\n",
      "Epoch 10/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - AUC: 0.7518 - loss: 0.2481\n",
      "Epoch 11/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - AUC: 0.7524 - loss: 0.2479\n",
      "Epoch 12/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.7529 - loss: 0.2477\n",
      "Epoch 13/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - AUC: 0.7536 - loss: 0.2475\n",
      "Epoch 14/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.7546 - loss: 0.2472\n",
      "Epoch 15/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.7546 - loss: 0.2471\n",
      "Epoch 16/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - AUC: 0.7557 - loss: 0.2468\n",
      "Epoch 17/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.7558 - loss: 0.2468\n",
      "Epoch 18/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - AUC: 0.7565 - loss: 0.2466\n",
      "Epoch 19/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.7567 - loss: 0.2464\n",
      "Epoch 20/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.7575 - loss: 0.2463\n",
      "Epoch 21/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - AUC: 0.7576 - loss: 0.2462\n",
      "Epoch 22/22\n",
      "\u001b[1m6649/6649\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - AUC: 0.7579 - loss: 0.2460\n",
      "\u001b[1m1922/1922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 983us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 10:26:28,478] Trial 3 finished with value: 0.7398958752261717 and parameters: {'epochs': 22, 'batch_size': 37, 'units_layer1': 20, 'units_layer2': 15, 'optimizer': 'adam', 'learning_rate': 0.0016960013230266206}. Best is trial 0 with value: 0.7444090078813803.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_12352\\3644950783.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5028 - loss: 0.7627\n",
      "Epoch 2/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - AUC: 0.5058 - loss: 0.7427\n",
      "Epoch 3/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5076 - loss: 0.7301\n",
      "Epoch 4/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5091 - loss: 0.7201\n",
      "Epoch 5/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5104 - loss: 0.7118\n",
      "Epoch 6/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5115 - loss: 0.7044\n",
      "Epoch 7/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5124 - loss: 0.6979\n",
      "Epoch 8/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - AUC: 0.5134 - loss: 0.6919\n",
      "Epoch 9/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - AUC: 0.5142 - loss: 0.6864\n",
      "Epoch 10/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5148 - loss: 0.6813\n",
      "Epoch 11/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5156 - loss: 0.6765\n",
      "Epoch 12/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5163 - loss: 0.6719\n",
      "Epoch 13/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5169 - loss: 0.6677\n",
      "Epoch 14/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5175 - loss: 0.6636\n",
      "Epoch 15/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5181 - loss: 0.6597\n",
      "Epoch 16/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5186 - loss: 0.6560\n",
      "Epoch 17/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5193 - loss: 0.6524\n",
      "Epoch 18/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - AUC: 0.5197 - loss: 0.6490\n",
      "Epoch 19/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5202 - loss: 0.6457\n",
      "Epoch 20/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5207 - loss: 0.6425\n",
      "Epoch 21/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - AUC: 0.5212 - loss: 0.6394\n",
      "Epoch 22/22\n",
      "\u001b[1m12301/12301\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - AUC: 0.5216 - loss: 0.6364\n",
      "\u001b[1m1922/1922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 907us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 10:33:50,356] Trial 4 finished with value: 0.5197208095763477 and parameters: {'epochs': 22, 'batch_size': 20, 'units_layer1': 13, 'units_layer2': 31, 'optimizer': 'adagrad', 'learning_rate': 1.2612451417992625e-05}. Best is trial 0 with value: 0.7444090078813803.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_12352\\3644950783.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3968/3968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7112 - loss: 0.2608\n",
      "Epoch 2/24\n",
      "\u001b[1m3968/3968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - AUC: 0.7204 - loss: 0.2577\n",
      "Epoch 3/24\n",
      "\u001b[1m3968/3968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7218 - loss: 0.2574\n",
      "Epoch 4/24\n",
      "\u001b[1m3968/3968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - AUC: 0.7228 - loss: 0.2571\n",
      "Epoch 5/24\n",
      "\u001b[1m3968/3968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7228 - loss: 0.2570\n",
      "Epoch 6/24\n",
      "\u001b[1m3968/3968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7212 - loss: 0.2577\n",
      "Epoch 7/24\n",
      "\u001b[1m3968/3968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.7219 - loss: 0.2579\n",
      "Epoch 8/24\n",
      "\u001b[1m3968/3968\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7222 - loss: 0.2583\n",
      "\u001b[1m1922/1922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 931us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 10:34:51,876] Trial 5 finished with value: 0.7213868672683317 and parameters: {'epochs': 24, 'batch_size': 62, 'units_layer1': 29, 'units_layer2': 26, 'optimizer': 'rmsprop', 'learning_rate': 0.004594508092831644}. Best is trial 0 with value: 0.7444090078813803.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_12352\\3644950783.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7140 - loss: 0.2622\n",
      "Epoch 2/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7377 - loss: 0.2525\n",
      "Epoch 3/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7418 - loss: 0.2513\n",
      "Epoch 4/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - AUC: 0.7440 - loss: 0.2507\n",
      "Epoch 5/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.7458 - loss: 0.2500\n",
      "Epoch 6/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - AUC: 0.7474 - loss: 0.2495\n",
      "Epoch 7/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7490 - loss: 0.2491\n",
      "Epoch 8/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - AUC: 0.7499 - loss: 0.2488\n",
      "Epoch 9/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - AUC: 0.7514 - loss: 0.2484\n",
      "Epoch 10/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7520 - loss: 0.2481\n",
      "Epoch 11/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7534 - loss: 0.2477\n",
      "Epoch 12/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7540 - loss: 0.2476\n",
      "Epoch 13/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - AUC: 0.7545 - loss: 0.2474\n",
      "Epoch 14/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7551 - loss: 0.2472\n",
      "Epoch 15/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.7559 - loss: 0.2470\n",
      "Epoch 16/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.7564 - loss: 0.2468\n",
      "Epoch 17/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - AUC: 0.7572 - loss: 0.2466\n",
      "Epoch 18/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7574 - loss: 0.2465\n",
      "Epoch 19/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.7581 - loss: 0.2463\n",
      "Epoch 20/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7583 - loss: 0.2462\n",
      "Epoch 21/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - AUC: 0.7592 - loss: 0.2460\n",
      "Epoch 22/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7593 - loss: 0.2459\n",
      "Epoch 23/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.7596 - loss: 0.2458\n",
      "Epoch 24/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7599 - loss: 0.2458\n",
      "Epoch 25/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - AUC: 0.7600 - loss: 0.2456\n",
      "Epoch 26/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - AUC: 0.7605 - loss: 0.2455\n",
      "Epoch 27/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.7611 - loss: 0.2453\n",
      "Epoch 28/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7610 - loss: 0.2453\n",
      "Epoch 29/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7613 - loss: 0.2452\n",
      "Epoch 30/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - AUC: 0.7617 - loss: 0.2451\n",
      "Epoch 31/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7619 - loss: 0.2451\n",
      "Epoch 32/32\n",
      "\u001b[1m4033/4033\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7622 - loss: 0.2449\n",
      "\u001b[1m1922/1922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 950us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 10:38:52,800] Trial 6 finished with value: 0.7376495925591009 and parameters: {'epochs': 32, 'batch_size': 61, 'units_layer1': 16, 'units_layer2': 28, 'optimizer': 'adam', 'learning_rate': 0.0013465971251822664}. Best is trial 0 with value: 0.7444090078813803.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_12352\\3644950783.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - AUC: 0.4947 - loss: 0.6303\n",
      "Epoch 2/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.4979 - loss: 0.5704\n",
      "Epoch 3/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.5005 - loss: 0.5381\n",
      "Epoch 4/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.5026 - loss: 0.5153\n",
      "Epoch 5/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5043 - loss: 0.4976\n",
      "Epoch 6/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.5059 - loss: 0.4832\n",
      "Epoch 7/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - AUC: 0.5074 - loss: 0.4712\n",
      "Epoch 8/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - AUC: 0.5086 - loss: 0.4609\n",
      "Epoch 9/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.5097 - loss: 0.4519\n",
      "Epoch 10/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - AUC: 0.5106 - loss: 0.4440\n",
      "Epoch 11/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step - AUC: 0.5116 - loss: 0.4369\n",
      "Epoch 12/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.5125 - loss: 0.4305\n",
      "Epoch 13/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 2ms/step - AUC: 0.5133 - loss: 0.4247\n",
      "Epoch 14/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.5141 - loss: 0.4195\n",
      "Epoch 15/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5148 - loss: 0.4146\n",
      "Epoch 16/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.5155 - loss: 0.4101\n",
      "Epoch 17/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.5161 - loss: 0.4060\n",
      "Epoch 18/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5169 - loss: 0.4022\n",
      "Epoch 19/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.5175 - loss: 0.3986\n",
      "Epoch 20/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.5181 - loss: 0.3952\n",
      "Epoch 21/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - AUC: 0.5187 - loss: 0.3920\n",
      "Epoch 22/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.5193 - loss: 0.3891\n",
      "Epoch 23/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.5199 - loss: 0.3863\n",
      "Epoch 24/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.5204 - loss: 0.3836\n",
      "Epoch 25/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 2ms/step - AUC: 0.5210 - loss: 0.3811\n",
      "Epoch 26/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.5215 - loss: 0.3787\n",
      "Epoch 27/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - AUC: 0.5221 - loss: 0.3765\n",
      "Epoch 28/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - AUC: 0.5226 - loss: 0.3743\n",
      "Epoch 29/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - AUC: 0.5231 - loss: 0.3723\n",
      "Epoch 30/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 2ms/step - AUC: 0.5237 - loss: 0.3703\n",
      "Epoch 31/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.5241 - loss: 0.3685\n",
      "Epoch 32/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - AUC: 0.5246 - loss: 0.3667\n",
      "Epoch 33/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.5251 - loss: 0.3650\n",
      "Epoch 34/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2ms/step - AUC: 0.5256 - loss: 0.3633\n",
      "Epoch 35/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - AUC: 0.5261 - loss: 0.3618\n",
      "Epoch 36/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - AUC: 0.5266 - loss: 0.3603\n",
      "Epoch 37/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 2ms/step - AUC: 0.5270 - loss: 0.3588\n",
      "Epoch 38/38\n",
      "\u001b[1m7936/7936\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 2ms/step - AUC: 0.5275 - loss: 0.3574\n",
      "\u001b[1m1922/1922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 10:48:09,100] Trial 7 finished with value: 0.5316643456362369 and parameters: {'epochs': 38, 'batch_size': 31, 'units_layer1': 15, 'units_layer2': 12, 'optimizer': 'adagrad', 'learning_rate': 6.929037750205947e-05}. Best is trial 0 with value: 0.7444090078813803.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_12352\\3644950783.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3905/3905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7105 - loss: 0.2613\n",
      "Epoch 2/50\n",
      "\u001b[1m3905/3905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - AUC: 0.7178 - loss: 0.2588\n",
      "Epoch 3/50\n",
      "\u001b[1m3905/3905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7183 - loss: 0.2591\n",
      "Epoch 4/50\n",
      "\u001b[1m3905/3905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7190 - loss: 0.2594\n",
      "Epoch 5/50\n",
      "\u001b[1m3905/3905\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - AUC: 0.7209 - loss: 0.2592\n",
      "\u001b[1m1922/1922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 884us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 10:48:45,746] Trial 8 finished with value: 0.7247444123145879 and parameters: {'epochs': 50, 'batch_size': 63, 'units_layer1': 29, 'units_layer2': 24, 'optimizer': 'rmsprop', 'learning_rate': 0.006937243749172697}. Best is trial 0 with value: 0.7444090078813803.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_12352\\3644950783.py:12: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.5487 - loss: 0.4690\n",
      "Epoch 2/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.5486 - loss: 0.4012\n",
      "Epoch 3/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - AUC: 0.5483 - loss: 0.3637\n",
      "Epoch 4/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.5492 - loss: 0.3417\n",
      "Epoch 5/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1ms/step - AUC: 0.5508 - loss: 0.3281\n",
      "Epoch 6/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.5531 - loss: 0.3192\n",
      "Epoch 7/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.5552 - loss: 0.3133\n",
      "Epoch 8/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.5580 - loss: 0.3090\n",
      "Epoch 9/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.5606 - loss: 0.3059\n",
      "Epoch 10/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.5634 - loss: 0.3035\n",
      "Epoch 11/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.5665 - loss: 0.3015\n",
      "Epoch 12/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.5694 - loss: 0.2998\n",
      "Epoch 13/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.5723 - loss: 0.2984\n",
      "Epoch 14/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.5753 - loss: 0.2971\n",
      "Epoch 15/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.5783 - loss: 0.2959\n",
      "Epoch 16/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.5813 - loss: 0.2949\n",
      "Epoch 17/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.5842 - loss: 0.2939\n",
      "Epoch 18/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.5871 - loss: 0.2929\n",
      "Epoch 19/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.5899 - loss: 0.2920\n",
      "Epoch 20/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1ms/step - AUC: 0.5927 - loss: 0.2912\n",
      "Epoch 21/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.5955 - loss: 0.2904\n",
      "Epoch 22/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.5980 - loss: 0.2896\n",
      "Epoch 23/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.6006 - loss: 0.2889\n",
      "Epoch 24/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.6032 - loss: 0.2881\n",
      "Epoch 25/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.6056 - loss: 0.2875\n",
      "Epoch 26/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.6081 - loss: 0.2868\n",
      "Epoch 27/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.6104 - loss: 0.2861\n",
      "Epoch 28/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.6127 - loss: 0.2855\n",
      "Epoch 29/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.6149 - loss: 0.2849\n",
      "Epoch 30/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.6171 - loss: 0.2844\n",
      "Epoch 31/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.6192 - loss: 0.2838\n",
      "Epoch 32/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.6213 - loss: 0.2833\n",
      "Epoch 33/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.6234 - loss: 0.2827\n",
      "Epoch 34/34\n",
      "\u001b[1m5592/5592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.6253 - loss: 0.2822\n",
      "\u001b[1m1922/1922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-11-11 10:54:11,195] Trial 9 finished with value: 0.6281408950350762 and parameters: {'epochs': 34, 'batch_size': 44, 'units_layer1': 28, 'units_layer2': 20, 'optimizer': 'sgd', 'learning_rate': 3.5972855659334694e-05}. Best is trial 0 with value: 0.7444090078813803.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: 0.7444090078813803\n",
      "Best hyperparameters: {'epochs': 22, 'batch_size': 56, 'units_layer1': 7, 'units_layer2': 32, 'optimizer': 'adam', 'learning_rate': 0.0002454790046811478}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "    \n",
    "def optimal(trial):\n",
    "    \n",
    "    # Suggest the number of epochs and batch size\n",
    "    epochs = trial.suggest_int('epochs', 10, 100)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 64)\n",
    "    \n",
    "    model = create_model(trial)\n",
    "    early_stopping = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, callbacks = [early_stopping])\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    return auc\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(optimal, n_trials=50)\n",
    "\n",
    "print(f\"Best trial: {study.best_trial.value}\")\n",
    "print(f\"Best hyperparameters: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "16fee22a-b6f2-4e0e-976e-5eb7bb84005a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 22,\n",
       " 'batch_size': 56,\n",
       " 'units_layer1': 7,\n",
       " 'units_layer2': 32,\n",
       " 'optimizer': 'adam',\n",
       " 'learning_rate': 0.0002454790046811478}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_params = study.best_trial.params\n",
    "\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "40d79f54-7c05-4dff-b26f-19c5aa381fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model with the best hyperparameters\n",
    "\n",
    "best_model = Sequential()\n",
    "best_model.add(Dense(units=best_params['units_layer1'], activation='relu'))\n",
    "best_model.add(Dense(units=best_params['units_layer2'], activation='relu'))\n",
    "best_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ed97a779-4cb1-4070-bd03-3217a6abe536",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_params['optimizer'] == 'adam':\n",
    "    best_optimizer = Adam(learning_rate=best_params['learning_rate'])\n",
    "elif best_params['optimizer'] == 'sgd':\n",
    "    best_optimizer = SGD(learning_rate=best_params['learning_rate'])\n",
    "elif best_params['optimizer'] == 'rmsprop':\n",
    "    best_optimizer = RMSprop(learning_rate=best_params['learning_rate'])\n",
    "elif best_params['optimizer'] == 'adagrad':\n",
    "    best_optimizer = Adagrad(learning_rate=best_params['learning_rate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0937e144-6382-46bd-9570-4eb1d908c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.compile(optimizer=best_optimizer, loss='binary_crossentropy', metrics=['AUC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7fa18a24-f7fe-4dfe-bda9-1deaad09b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=best_params['batch_size'])\n",
    "    \n",
    "    '''Predictions and probabilities for the training set'''\n",
    "    \n",
    "    y_train_prob = model.predict(X_train)\n",
    "\n",
    "    '''Predictions and probabilities for the test set'''\n",
    "    \n",
    "    y_test_prob = model.predict(X_test)\n",
    "\n",
    "    '''Calculate metrics for the training set''' \n",
    "    \n",
    "    roc_train_prob = roc_auc_score(y_train, y_train_prob)\n",
    "    gini_train_prob = roc_train_prob * 2 - 1\n",
    "    \n",
    "\n",
    "    '''Calculate metrics for the test set'''\n",
    "    \n",
    "    roc_test_prob = roc_auc_score(y_test, y_test_prob)\n",
    "    gini_test_prob = roc_test_prob * 2 - 1\n",
    "    \n",
    "\n",
    "    results = pd.DataFrame({\n",
    "        'Dataset': ['Train', 'Test'],\n",
    "        'Gini': [gini_train_prob * 100, gini_test_prob * 100],\n",
    "    \n",
    "    })\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "6626d7f8-63ba-4294-86da-c16b6e646be5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - AUC: 0.6536 - loss: 0.2940\n",
      "Epoch 2/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - AUC: 0.7274 - loss: 0.2561\n",
      "Epoch 3/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7350 - loss: 0.2535\n",
      "Epoch 4/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1ms/step - AUC: 0.7390 - loss: 0.2523\n",
      "Epoch 5/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7414 - loss: 0.2515\n",
      "Epoch 6/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7431 - loss: 0.2509\n",
      "Epoch 7/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7445 - loss: 0.2506\n",
      "Epoch 8/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7452 - loss: 0.2503\n",
      "Epoch 9/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7463 - loss: 0.2500\n",
      "Epoch 10/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7469 - loss: 0.2498\n",
      "Epoch 11/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7473 - loss: 0.2497\n",
      "Epoch 12/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7477 - loss: 0.2495\n",
      "Epoch 13/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7483 - loss: 0.2494\n",
      "Epoch 14/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7487 - loss: 0.2493\n",
      "Epoch 15/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7488 - loss: 0.2492\n",
      "Epoch 16/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 2ms/step - AUC: 0.7494 - loss: 0.2491\n",
      "Epoch 17/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7492 - loss: 0.2490\n",
      "Epoch 18/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7496 - loss: 0.2489\n",
      "Epoch 19/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7500 - loss: 0.2488\n",
      "Epoch 20/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 2ms/step - AUC: 0.7502 - loss: 0.2487\n",
      "Epoch 21/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7504 - loss: 0.2487\n",
      "Epoch 22/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7508 - loss: 0.2486\n",
      "Epoch 23/23\n",
      "\u001b[1m4393/4393\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - AUC: 0.7510 - loss: 0.2485\n",
      "\u001b[1m7688/7688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 943us/step\n",
      "\u001b[1m1922/1922\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 939us/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Gini</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train</td>\n",
       "      <td>50.688438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Test</td>\n",
       "      <td>48.644477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Dataset       Gini\n",
       "0   Train  50.688438\n",
       "1    Test  48.644477"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(best_model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e8cf7b-1c41-49fe-bc59-578438feb837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
